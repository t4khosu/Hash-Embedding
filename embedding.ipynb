{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example on how Bloom / Hash Embedding works\n",
    "\n",
    "This embedding uses single features of its inputs to create an embedding.\n",
    "Each feature can be represented by an vector with M elements. The sum of all\n",
    "features generates the input vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mmh3\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, with need embeddings for 20 words:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 110
    }
   ],
   "source": [
    "nouns = ['bunch', 'audience', 'flock', 'team', 'group', 'family', \n",
    "         'band', 'village', 'cat', 'sock', 'ship', 'hero',\n",
    "         'monkey', 'baby', 'match', 'bed', 'dog', 'bottle', 'house',\n",
    "         'paper']\n",
    "\n",
    "len(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's say we choose 10 feature vectors, each with 2 elements, that can represent\n",
    "a single word:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[-0.08058126 -0.03549474]\n [-0.02258513 -0.02622956]\n [-0.04844884  0.08611142]\n [-0.02237704  0.08822986]\n [-0.01117155 -0.00470919]\n [ 0.01774416 -0.03864823]\n [-0.0529064  -0.08787051]\n [-0.00538983 -0.00827288]\n [ 0.01222911  0.05121812]\n [ 0.01081429  0.06178878]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "num_features = 10\n",
    "feature_size = 2\n",
    "feature_vecs = np.random.uniform(-0.1, 0.1, (num_features, feature_size))\n",
    "\n",
    "print(feature_vecs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to define a rule, that maps **one** input onto **M** of these vectors.\n",
    "Normally we would check for specific word features (POS, position in text, ...).\n",
    "In this example we use Hashes to map words onto a range from `0` to `num_features - 1`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Inividual features for a: 7\nInividual features for b: 9\nInividual features for c: 10\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "feature_keys_a = [mmh3.hash(n, 1) % num_features for n in nouns]\n",
    "feature_keys_b = [mmh3.hash(n, 2) % num_features for n in nouns]\n",
    "feature_keys_c = [mmh3.hash(n, 3) % num_features for n in nouns]\n",
    "\n",
    "print(f'Inividual features for a: {len(set(feature_keys_a))}')\n",
    "print(f'Inividual features for b: {len(set(feature_keys_b))}')\n",
    "print(f'Inividual features for c: {len(set(feature_keys_c))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, one feature alone can't differentiate between every single word.\n",
    "There are collisions! Because of this we combine our features. This way, each word is defined\n",
    "by three features:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Feature keys for first two inputs: [(3, 7, 1), (6, 0, 3)]\nIndividual combined features: 20\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "feature_keys = list(zip(\n",
    "    feature_keys_a,\n",
    "    feature_keys_b,\n",
    "    feature_keys_c,\n",
    "))\n",
    "\n",
    "print(f'Feature keys for first two inputs: {feature_keys[:2]}')\n",
    "print(f'Individual combined features: {len(set(feature_keys))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see, every word is represented by an individual tuple of keys.\n",
    "In the last step we can search for the corresponding features and add them up:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for keys in feature_keys:\n",
    "    emb = sum(feature_vecs[k] for k in keys)\n",
    "    embeddings.append(emb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "bunch -> [-0.050352    0.05372742]\naudience -> [-0.1558647  -0.03513538]\nflock -> [-0.08361109 -0.04040732]\nteam -> [-0.09136091 -0.0520405 ]\ngroup -> [-0.01002271  0.04130874]\nfamily -> [0.00144945 0.03467235]\nband -> [-0.05774738 -0.1527483 ]\nvillage -> [-0.04606712 -0.04492528]\ncat -> [-0.02152758  0.02027937]\nsock -> [-0.14020165  0.0459075 ]\nship -> [-0.14980408  0.08435234]\nhero -> [ 0.04630261 -0.01550768]\nmonkey -> [-0.09786857 -0.0258702 ]\nbaby -> [-0.02721801  0.02335207]\nmatch -> [-0.04055207 -0.13479163]\nbed -> [-0.16655234 -0.07926236]\ndog -> [-0.11698436 -0.18045021]\nbottle -> [-0.0592285   0.06956565]\nhouse -> [-0.09320292  0.26257114]\npaper -> [-0.05326366 -0.03079092]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for word, emb in zip(nouns, embeddings):\n",
    "    print(f'{word} -> {emb}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now each word has it's own representation, even though we only store 10 feature vectors!\n",
    "However, this example has one flaw: Adding an unknown word may be possible, but it is dependent \n",
    "on a random hash function. So even if we train a model on these words, results for new\n",
    "won't make any sense.\n",
    "\n",
    "For example: We use this method to check if a word is a noun or verb. We may be able to train a model,\n",
    "but for new words arbitrary hash values will be generated to get feature vectors. So it is not possible\n",
    "to say if an unknown word is a verb or noun.\n",
    "\n",
    "But what we can test is training a model on the known words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0 0.2584920620682681\n1 0.20421630834396456\n2 0.17985221189984488\n3 0.16256476557363916\n4 0.15651477342069486\n5 0.1539980470577015\n6 0.14941944216323205\n7 0.15495934058180125\n8 0.13974220357689598\n9 0.14000019907864833\n10 0.14364069543255248\n11 0.14127378651259745\n12 0.14851220255985334\n13 0.14295166947298749\n14 0.1423234540300131\n15 0.13905704788288586\n16 0.14183426626600465\n17 0.14214815417937154\n18 0.14772398381702004\n19 0.14881169898958704\n20 0.14152221190918393\n21 0.14177297145108703\n22 0.14733553376470787\n23 0.1467140777521138\n24 0.14022723117311342\n25 0.13893245794794154\n26 0.14065391558903745\n27 0.1448032074734772\n28 0.13824316863757646\n29 0.15130812397214255\n30 0.15009535187104273\n31 0.14734792907675529\n32 0.1470798201533985\n33 0.14647412810438826\n34 0.1417883194458527\n35 0.1472935013360437\n36 0.14658727754053555\n37 0.14092185880827804\n38 0.14400377664746145\n39 0.14203383679365206\n40 0.14495631797750988\n41 0.14465548109431137\n42 0.14027696147487811\n43 0.1511848547541024\n44 0.14427969410659047\n45 0.15027894806543046\n46 0.144783831487404\n47 0.13419599927858336\n48 0.13008919105352626\n49 0.1438538325135074\n50 0.14876698794727225\n51 0.14792035125245798\n52 0.14556910949758778\n53 0.1488625307428503\n54 0.1343843554027161\n55 0.154722051668598\n56 0.14575221762329338\n57 0.14038462065113022\n58 0.14719234990986577\n59 0.1419856482778113\n60 0.14732637641543775\n61 0.14935278883448375\n62 0.14523640992854567\n63 0.14828111969298566\n64 0.1439770700920902\n65 0.14407484280949198\n66 0.14650944075405198\n67 0.1399945032581482\n68 0.13714563225700324\n69 0.14464265186798292\n70 0.14469993176332294\n71 0.14021982364060256\n72 0.14795930264013527\n73 0.14975033902556054\n74 0.14332758038821697\n75 0.13745780494385657\n76 0.14854232643425996\n77 0.14564349568825122\n78 0.14486632612821074\n79 0.14634760104999478\n80 0.1406325694241053\n81 0.14487595787841406\n82 0.14062655608904157\n83 0.14825970926649779\n84 0.14391402049774574\n85 0.14825973078246904\n86 0.1385526308107801\n87 0.14473972500145627\n88 0.14616716928837312\n89 0.13929289092086905\n90 0.15023954066064873\n91 0.13211070146974713\n92 0.14507407458110932\n93 0.1431988648168385\n94 0.14540104021848718\n95 0.1474138610361164\n96 0.14905199709720685\n97 0.14819268763244572\n98 0.14218276195494164\n99 0.1403701325577105\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "nb_epoch = 100\n",
    "learn_rate = 0.1\n",
    "    \n",
    "labels = np.random.uniform(-0.1, 0.1, (len(nouns), 2))\n",
    "examples = list(zip(nouns, feature_keys, labels))\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    random.shuffle(examples)\n",
    "    loss=0.0\n",
    "\n",
    "    for noun, keys, label in examples:\n",
    "        hash_vector = sum(feature_vecs[k] for k in keys)\n",
    "\n",
    "        diff = hash_vector - label\n",
    "\n",
    "        for k in keys:\n",
    "            feature_vecs[k] -= learn_rate * diff\n",
    "\n",
    "        loss += (diff**2).sum()\n",
    "    print(epoch, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hash_embedding",
   "language": "python",
   "display_name": "hash_embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}